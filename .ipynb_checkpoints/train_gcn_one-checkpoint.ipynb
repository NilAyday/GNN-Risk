{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "710b2042-116e-4a8f-92f3-74136939e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from model import *\n",
    "from process import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "919d22b8-92d4-405d-9f30-267db9c11464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708])\n",
      "torch.Size([140])\n",
      "torch.Size([500])\n",
      "torch.Size([1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayday\\Desktop\\Master_Thesis\\experiment\\utils.py:87: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels,idx_train,idx_val,idx_test = load_citation('cora')\n",
    "data='cora'\n",
    "#splitstr = 'splits/'+data+'_split_0.6_0.2_'+str(0)+'.npz'\n",
    "#adj, features, labels, idx_train, idx_val, idx_test, num_features, num_labels = full_load_data('cora',splitstr)\n",
    "print(adj.shape)\n",
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "print(idx_train.shape)\n",
    "print(idx_val.shape)\n",
    "print(idx_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3193118e-5536-4c60-8ac6-d4ea6ebb5602",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudaid = \"cuda\"\n",
    "device = torch.device(cudaid)\n",
    "features = features.to(device)\n",
    "adj = adj.to(device).coalesce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98df0ec8-26ee-4028-abf5-d8401f636e8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.4, 'nhid_list': [128, 512, 128], 'accuracy': 0.792}\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model\u001b[38;5;241m=\u001b[39mGCN(\u001b[43mn\u001b[49m,feature_size,[\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m128\u001b[39m],num_classes,dropout)\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m,weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a630290-eeb5-4007-a441-042e960ada79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_GraphConvolution(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, in_features, out_features,nfeat,n, bias=True):\n",
    "        super(my_GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features+nfeat, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj,x):\n",
    "        \n",
    "        \n",
    "        #support = torch.cat((input-torch.mm(adj.to_dense(), input), adj.to_dense()),1)\n",
    "        \n",
    "        support = torch.cat((x,torch.mm(adj.to_dense(), input) + input),1)\n",
    "        output = torch.mm(support, self.weight)\n",
    "\n",
    "        #output= torch.mm(torch.mm(adj.to_dense(), input) + input, self.weight)\n",
    "\n",
    "       \n",
    "        \n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class my_GCN(nn.Module):\n",
    "    def __init__(self, n,nfeat, nhid_list, nclass, dropout):\n",
    "        super(my_GCN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        if nhid_list:\n",
    "            # Input layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nhid_list[0],nfeat,n))\n",
    "            \n",
    "            # Hidden layers\n",
    "            for i in range(1, len(nhid_list)):\n",
    "                self.layers.append(my_GraphConvolution(nhid_list[i-1], nhid_list[i],nfeat,n))\n",
    "            \n",
    "            # Output layer\n",
    "            self.layers.append(my_GraphConvolution(nhid_list[-1], nclass,nfeat,n))\n",
    "        else:\n",
    "            # Single output layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nclass,nfeat,n))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        h = x.detach().requires_grad_()\n",
    "        \n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            h = F.relu(layer(h, adj,x))\n",
    "            h = F.dropout(h, self.dropout, training=self.training)\n",
    "        \n",
    "        h = self.layers[-1](h, adj,x)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a80b32c3-cfe2-44a4-b5ca-c7d9561e1293",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size=features.shape[1]\n",
    "num_classes = len(torch.unique(labels))\n",
    "n=features.shape[0]\n",
    "\n",
    "dropout=0.4\n",
    "\n",
    "#{'lr': 0.001, 'weight_decay': 0.001, 'dropout': 0.4, 'nhid_list': [128, 512, 128], 'accuracy': 0.812}\n",
    "#{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.4, 'nhid_list': [128, 512, 128], 'accuracy': 0.792}\n",
    "model=my_GCN(n,feature_size,[128, 512, 128],num_classes,dropout)\n",
    "model=model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.001)\n",
    "\n",
    "epochs=100\n",
    "patience=10\n",
    "test=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6fd08d6-7c9a-4d29-bcb2-2da5855f0b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0001 train loss:2.001 acc:15.71 | val loss:1.936 acc:9.00\n",
      "Epoch:0002 train loss:1.927 acc:15.00 | val loss:1.953 acc:9.20\n",
      "Epoch:0003 train loss:1.857 acc:30.00 | val loss:1.927 acc:12.60\n",
      "Epoch:0004 train loss:1.859 acc:25.00 | val loss:1.887 acc:22.60\n",
      "Epoch:0005 train loss:1.773 acc:44.29 | val loss:1.846 acc:38.60\n",
      "Epoch:0006 train loss:1.726 acc:47.14 | val loss:1.800 acc:41.80\n",
      "Epoch:0007 train loss:1.716 acc:47.86 | val loss:1.749 acc:49.20\n",
      "Epoch:0008 train loss:1.636 acc:57.86 | val loss:1.698 acc:64.00\n",
      "Epoch:0009 train loss:1.545 acc:64.29 | val loss:1.654 acc:71.20\n",
      "Epoch:0010 train loss:1.446 acc:77.14 | val loss:1.615 acc:71.60\n",
      "Epoch:0011 train loss:1.384 acc:75.00 | val loss:1.578 acc:69.20\n",
      "Epoch:0012 train loss:1.270 acc:87.14 | val loss:1.515 acc:70.00\n",
      "Epoch:0013 train loss:1.167 acc:90.00 | val loss:1.435 acc:73.40\n",
      "Epoch:0014 train loss:1.039 acc:90.00 | val loss:1.351 acc:76.00\n",
      "Epoch:0015 train loss:0.924 acc:90.71 | val loss:1.263 acc:76.20\n",
      "Epoch:0016 train loss:0.850 acc:87.86 | val loss:1.172 acc:77.80\n",
      "Epoch:0017 train loss:0.753 acc:92.14 | val loss:1.098 acc:78.00\n",
      "Epoch:0018 train loss:0.615 acc:91.43 | val loss:1.039 acc:78.40\n",
      "Epoch:0019 train loss:0.513 acc:94.29 | val loss:0.983 acc:78.00\n",
      "Epoch:0020 train loss:0.459 acc:96.43 | val loss:0.925 acc:78.00\n",
      "Epoch:0021 train loss:0.402 acc:93.57 | val loss:0.867 acc:78.00\n",
      "Epoch:0022 train loss:0.354 acc:97.14 | val loss:0.807 acc:79.20\n",
      "Epoch:0023 train loss:0.221 acc:99.29 | val loss:0.752 acc:79.20\n",
      "Epoch:0024 train loss:0.217 acc:97.14 | val loss:0.712 acc:79.60\n",
      "Epoch:0025 train loss:0.161 acc:97.86 | val loss:0.692 acc:79.80\n",
      "Epoch:0026 train loss:0.141 acc:98.57 | val loss:0.691 acc:79.00\n",
      "Epoch:0027 train loss:0.122 acc:97.86 | val loss:0.696 acc:79.80\n",
      "Epoch:0028 train loss:0.091 acc:99.29 | val loss:0.710 acc:79.60\n",
      "Epoch:0029 train loss:0.082 acc:100.00 | val loss:0.723 acc:79.00\n",
      "Epoch:0030 train loss:0.058 acc:100.00 | val loss:0.726 acc:78.40\n",
      "Epoch:0031 train loss:0.066 acc:98.57 | val loss:0.715 acc:78.40\n",
      "Epoch:0032 train loss:0.048 acc:99.29 | val loss:0.702 acc:79.40\n",
      "Epoch:0033 train loss:0.034 acc:100.00 | val loss:0.703 acc:79.20\n",
      "Epoch:0034 train loss:0.029 acc:100.00 | val loss:0.701 acc:79.40\n",
      "Epoch:0035 train loss:0.022 acc:100.00 | val loss:0.700 acc:79.00\n",
      "Epoch:0036 train loss:0.034 acc:99.29 | val loss:0.721 acc:79.80\n",
      "Train cost: 2.0838s\n",
      "Load 25th epoch\n",
      "Val acc.:80.6\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features,adj)\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train].to(device))\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train].to(device))\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    return loss_train.item(),acc_train.item()\n",
    "\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features,adj)\n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val].to(device))\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val].to(device))\n",
    "        return loss_val.item(),acc_val.item()\n",
    "\n",
    "def test():\n",
    "    #model.load_state_dict(torch.load(checkpt_file))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features, adj)\n",
    "        loss_test = F.nll_loss(output[idx_test], labels[idx_test].to(device))\n",
    "        acc_test = accuracy(output[idx_test], labels[idx_test].to(device))\n",
    "        return loss_test.item(),acc_test.item()\n",
    "    \n",
    "t_total = time.time()\n",
    "bad_counter = 0\n",
    "best = 999999999\n",
    "best_epoch = 0\n",
    "acc = 0\n",
    "for epoch in range(epochs):\n",
    "    loss_tra,acc_tra = train()\n",
    "    loss_val,acc_val = validate()\n",
    "    if(epoch+1)%1 == 0: \n",
    "        print('Epoch:{:04d}'.format(epoch+1),\n",
    "            'train',\n",
    "            'loss:{:.3f}'.format(loss_tra),\n",
    "            'acc:{:.2f}'.format(acc_tra*100),\n",
    "            '| val',\n",
    "            'loss:{:.3f}'.format(loss_val),\n",
    "            'acc:{:.2f}'.format(acc_val*100))\n",
    "    if loss_val < best:\n",
    "        best = loss_val\n",
    "        best_epoch = epoch\n",
    "        acc = acc_val\n",
    "        #torch.save(model.state_dict(), checkpt_file)\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1\n",
    "\n",
    "    if bad_counter == patience:\n",
    "        break\n",
    "\n",
    "if test:\n",
    "    acc = test()[1]\n",
    "\n",
    "print(\"Train cost: {:.4f}s\".format(time.time() - t_total))\n",
    "print('Load {}th epoch'.format(best_epoch))\n",
    "print(\"Val\",\"acc.:{:.1f}\".format(acc*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9badfed2-6e6b-4754-ac92-63aea07c60d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
