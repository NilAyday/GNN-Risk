{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62169ce6-d3ff-486e-8af5-4fc9a5c00eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from model import *\n",
    "from process import *\n",
    "import pickle\n",
    "import itertools\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ddbba5-f6a5-4367-837b-bcdea631a5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708])\n",
      "torch.Size([140])\n",
      "torch.Size([500])\n",
      "torch.Size([1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayday\\Desktop\\Master_Thesis\\experiment\\utils.py:87: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n"
     ]
    }
   ],
   "source": [
    "data='cora'\n",
    "adj, features, labels,idx_train,idx_val,idx_test = load_citation(data)\n",
    "#splitstr = 'splits/'+data+'_split_0.6_0.2_'+str(0)+'.npz'\n",
    "#adj, features, labels, idx_train, idx_val, idx_test, num_features, num_labels = full_load_data(data,splitstr)\n",
    "print(adj.shape)\n",
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "print(idx_train.shape)\n",
    "print(idx_val.shape)\n",
    "print(idx_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f516b9ec-672b-4a67-99b1-bbf1323a7f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudaid = \"cuda\"\n",
    "device = torch.device(cudaid)\n",
    "features = features.to(device)\n",
    "adj = adj.to(device).coalesce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42522538-1af7-4041-894c-3100f095ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=features.shape[0]\n",
    "feature_size=features.shape[1]\n",
    "num_classes = len(torch.unique(labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92e25c24-4159-4e80-92d0-987042226f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_GraphConvolution(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, in_features, out_features,nfeat, bias=True):\n",
    "        super(my_GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features+n+nfeat, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj,x):\n",
    "        \n",
    "        \n",
    "        #support = torch.cat((input-torch.mm(adj.to_dense(), input), adj.to_dense()),1)\n",
    "        \n",
    "        support = torch.cat((torch.mm(adj.to_dense(), input),adj.to_dense(),x),1)\n",
    "        output = torch.mm(support, self.weight)\n",
    "\n",
    "        #output= torch.mm(torch.mm(adj.to_dense(), input) + input, self.weight)\n",
    "\n",
    "       \n",
    "        \n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "        \n",
    "class my_GCN(nn.Module):\n",
    "    def __init__(self, n,nfeat, nhid_list, nclass, dropout):\n",
    "        super(my_GCN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        if nhid_list:\n",
    "            # Input layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nhid_list[0],nfeat))\n",
    "            \n",
    "            # Hidden layers\n",
    "            for i in range(1, len(nhid_list)):\n",
    "                self.layers.append(my_GraphConvolution(nhid_list[i-1], nhid_list[i],nfeat))\n",
    "            \n",
    "            # Output layer\n",
    "            self.layers.append(my_GraphConvolution(nhid_list[-1], nclass,nfeat))\n",
    "        else:\n",
    "            # Single output layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nclass,nfeat))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        h = x.detach().requires_grad_()\n",
    "        \n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            h = F.relu(layer(h, adj,x))\n",
    "            h = F.dropout(h, self.dropout, training=self.training)\n",
    "        \n",
    "        h = self.layers[-1](h, adj,x)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "model=my_GCN(n,feature_size,[512,256],num_classes,dropout=0.5)\n",
    "model=model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.1,weight_decay=5e-4)\n",
    "\n",
    "epochs=100\n",
    "patience=10\n",
    "test=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82ee82cd-1ba7-4682-8c7f-2a8ff358a71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_GCN(\n",
       "  (layers): ModuleList(\n",
       "    (0): my_GraphConvolution (1433 -> 512)\n",
       "    (1): my_GraphConvolution (512 -> 7)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ddf9e80-fea2-443e-93b7-ded05b3e327d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ffcf9578-d717-4873-a300-cfd4d01f74b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0001 train loss:2.006 acc:15.00 | val loss:27.952 acc:5.80\n",
      "Epoch:0002 train loss:30.248 acc:15.71 | val loss:3.002 acc:22.40\n",
      "Epoch:0003 train loss:5.137 acc:33.57 | val loss:2.377 acc:26.80\n",
      "Epoch:0004 train loss:1.925 acc:51.43 | val loss:1.543 acc:27.60\n",
      "Epoch:0005 train loss:0.630 acc:82.86 | val loss:1.334 acc:58.20\n",
      "Epoch:0006 train loss:0.273 acc:98.57 | val loss:1.279 acc:60.40\n",
      "Epoch:0007 train loss:0.134 acc:100.00 | val loss:1.179 acc:65.80\n",
      "Epoch:0008 train loss:0.091 acc:100.00 | val loss:1.111 acc:65.20\n",
      "Epoch:0009 train loss:0.062 acc:100.00 | val loss:1.210 acc:62.40\n",
      "Epoch:0010 train loss:0.034 acc:100.00 | val loss:1.562 acc:50.80\n",
      "Epoch:0011 train loss:0.045 acc:100.00 | val loss:1.175 acc:65.60\n",
      "Epoch:0012 train loss:0.024 acc:100.00 | val loss:1.030 acc:64.40\n",
      "Epoch:0013 train loss:0.032 acc:100.00 | val loss:1.388 acc:51.60\n",
      "Epoch:0014 train loss:0.038 acc:100.00 | val loss:1.446 acc:57.00\n",
      "Epoch:0015 train loss:0.056 acc:99.29 | val loss:1.018 acc:64.40\n",
      "Epoch:0016 train loss:0.024 acc:100.00 | val loss:0.998 acc:63.80\n",
      "Epoch:0017 train loss:0.035 acc:100.00 | val loss:1.164 acc:64.80\n",
      "Epoch:0018 train loss:0.025 acc:100.00 | val loss:1.397 acc:60.20\n",
      "Epoch:0019 train loss:0.031 acc:100.00 | val loss:1.195 acc:62.00\n",
      "Epoch:0020 train loss:0.030 acc:100.00 | val loss:0.854 acc:73.20\n",
      "Epoch:0021 train loss:0.048 acc:99.29 | val loss:1.374 acc:59.00\n",
      "Epoch:0022 train loss:0.056 acc:99.29 | val loss:1.269 acc:65.60\n",
      "Epoch:0023 train loss:0.069 acc:97.86 | val loss:1.127 acc:67.80\n",
      "Epoch:0024 train loss:0.038 acc:98.57 | val loss:1.115 acc:70.00\n",
      "Epoch:0025 train loss:0.070 acc:97.86 | val loss:0.987 acc:71.80\n",
      "Epoch:0026 train loss:0.036 acc:99.29 | val loss:1.290 acc:68.40\n",
      "Epoch:0027 train loss:0.043 acc:99.29 | val loss:1.486 acc:63.00\n",
      "Epoch:0028 train loss:0.062 acc:97.86 | val loss:1.060 acc:72.00\n",
      "Epoch:0029 train loss:0.030 acc:99.29 | val loss:1.092 acc:70.80\n",
      "Epoch:0030 train loss:0.028 acc:99.29 | val loss:1.098 acc:72.00\n",
      "Train cost: 2.3359s\n",
      "Load 19th epoch\n",
      "Val acc.:73.0\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features,adj)\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train].to(device))\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train].to(device))\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    return loss_train.item(),acc_train.item()\n",
    "\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features,adj)\n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val].to(device))\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val].to(device))\n",
    "        return loss_val.item(),acc_val.item()\n",
    "\n",
    "def test():\n",
    "    #model.load_state_dict(torch.load(checkpt_file))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features, adj)\n",
    "        loss_test = F.nll_loss(output[idx_test], labels[idx_test].to(device))\n",
    "        acc_test = accuracy(output[idx_test], labels[idx_test].to(device))\n",
    "        return loss_test.item(),acc_test.item()\n",
    "    \n",
    "t_total = time.time()\n",
    "bad_counter = 0\n",
    "best = 999999999\n",
    "best_epoch = 0\n",
    "acc = 0\n",
    "for epoch in range(epochs):\n",
    "    loss_tra,acc_tra = train()\n",
    "    loss_val,acc_val = validate()\n",
    "    if(epoch+1)%1 == 0: \n",
    "        print('Epoch:{:04d}'.format(epoch+1),\n",
    "            'train',\n",
    "            'loss:{:.3f}'.format(loss_tra),\n",
    "            'acc:{:.2f}'.format(acc_tra*100),\n",
    "            '| val',\n",
    "            'loss:{:.3f}'.format(loss_val),\n",
    "            'acc:{:.2f}'.format(acc_val*100))\n",
    "    if loss_val < best:\n",
    "        best = loss_val\n",
    "        best_epoch = epoch\n",
    "        acc = acc_val\n",
    "        #torch.save(model.state_dict(), checkpt_file)\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1\n",
    "\n",
    "    if bad_counter == patience:\n",
    "        break\n",
    "\n",
    "if test:\n",
    "    acc = test()[1]\n",
    "\n",
    "print(\"Train cost: {:.4f}s\".format(time.time() - t_total))\n",
    "print('Load {}th epoch'.format(best_epoch))\n",
    "print(\"Val\",\"acc.:{:.1f}\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e6477916-3c0a-42a5-925c-7ca5bd3c39e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters and accuracies sorted by accuracy:\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [128, 256, 128], 'accuracy': 0.786}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [256, 512], 'accuracy': 0.78}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [256, 512], 'accuracy': 0.778}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [512, 1024, 512], 'accuracy': 0.778}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [64, 128, 64], 'accuracy': 0.776}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [128, 256, 128], 'accuracy': 0.776}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [64, 128, 64], 'accuracy': 0.774}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [64, 128, 64], 'accuracy': 0.772}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [512, 1024, 512], 'accuracy': 0.772}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [512, 1024, 512], 'accuracy': 0.772}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [512, 1024, 512], 'accuracy': 0.77}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [128, 256, 128], 'accuracy': 0.77}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [16, 32, 16], 'accuracy': 0.77}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [128, 256, 128], 'accuracy': 0.768}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [256, 512], 'accuracy': 0.768}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [16, 32, 16], 'accuracy': 0.766}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [128, 256, 128], 'accuracy': 0.766}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [256, 512], 'accuracy': 0.766}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [512, 1024, 512], 'accuracy': 0.762}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [64, 128, 64], 'accuracy': 0.76}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [64, 128, 64], 'accuracy': 0.758}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [64, 128, 64], 'accuracy': 0.756}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [64, 128, 64], 'accuracy': 0.756}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [128, 256, 128], 'accuracy': 0.756}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [256, 512], 'accuracy': 0.756}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [256, 512], 'accuracy': 0.754}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [16, 32, 16], 'accuracy': 0.75}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [512, 1024, 512], 'accuracy': 0.748}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [512, 1024, 512], 'accuracy': 0.748}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [128, 256, 128], 'accuracy': 0.746}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [64, 128, 64], 'accuracy': 0.746}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [128, 256, 128], 'accuracy': 0.746}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [256, 512], 'accuracy': 0.744}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [16, 32, 16], 'accuracy': 0.744}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [256, 512], 'accuracy': 0.742}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [512, 1024, 512], 'accuracy': 0.738}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [], 'accuracy': 0.718}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [], 'accuracy': 0.718}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [16, 32, 16], 'accuracy': 0.716}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [], 'accuracy': 0.714}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [], 'accuracy': 0.712}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [16, 32, 16], 'accuracy': 0.706}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [], 'accuracy': 0.7000000000000001}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [], 'accuracy': 0.6960000000000001}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [], 'accuracy': 0.6880000000000001}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [16, 32, 16], 'accuracy': 0.686}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [], 'accuracy': 0.67}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [16, 32, 16], 'accuracy': 0.652}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "# Assuming my_GraphConvolution is already defined somewhere\n",
    "\n",
    "class my_GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid_list, nclass, dropout):\n",
    "        super(my_GCN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        if nhid_list:\n",
    "            # Input layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nhid_list[0],nfeat))\n",
    "            \n",
    "            # Hidden layers\n",
    "            for i in range(1, len(nhid_list)):\n",
    "                self.layers.append(my_GraphConvolution(nhid_list[i-1], nhid_list[i],nfeat))\n",
    "            \n",
    "            # Output layer\n",
    "            self.layers.append(my_GraphConvolution(nhid_list[-1], nclass,nfeat))\n",
    "        else:\n",
    "            # Single output layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nclass,nfeat))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        h = x.detach().requires_grad_()\n",
    "        \n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            h = F.relu(layer(h, adj,x))\n",
    "            h = F.dropout(h, self.dropout, training=self.training)\n",
    "        \n",
    "        h = self.layers[-1](h, adj,x)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)\n",
    "\n",
    "# Assuming accuracy function is already defined\n",
    "\n",
    "def train(model, optimizer, features, adj, labels, idx_train):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train].to(device))\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train].to(device))\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    return loss_train.item(), acc_train.item()\n",
    "\n",
    "def validate(model, features, adj, labels, idx_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features, adj)\n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val].to(device))\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val].to(device))\n",
    "        return loss_val.item(), acc_val.item()\n",
    "\n",
    "def test(model, features, adj, labels, idx_test, checkpt_file):\n",
    "    model.load_state_dict(torch.load(checkpt_file))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features, adj)\n",
    "        loss_test = F.nll_loss(output[idx_test], labels[idx_test].to(device))\n",
    "        acc_test = accuracy(output[idx_test], labels[idx_test].to(device))\n",
    "        return loss_test.item(), acc_test.item()\n",
    "\n",
    "# Hyperparameters\n",
    "nfeat = features.shape[1]\n",
    "nclass = num_classes\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hyperparams = {\n",
    "    'lr': [0.01, 0.005],\n",
    "    'weight_decay': [5e-4, 5e-3],\n",
    "    'dropout': [0.5, 0.3],\n",
    "    'nhid_list': [[], [256, 512], [16, 32, 16], [64, 128, 64], [128, 256, 128], [512, 1024, 512]]\n",
    "\n",
    "}\n",
    "\n",
    "# To store results\n",
    "results = []\n",
    "\n",
    "for lr, weight_decay, dropout, nhid_list in itertools.product(\n",
    "    hyperparams['lr'], hyperparams['weight_decay'], hyperparams['dropout'], hyperparams['nhid_list']\n",
    "):\n",
    "    model = my_GCN(nfeat, nhid_list, nclass, dropout)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    epochs = 100\n",
    "    patience = 10\n",
    "    best = 999999999\n",
    "    best_epoch = 0\n",
    "    acc = 0\n",
    "    t_total = time.time()\n",
    "    bad_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss_tra, acc_tra = train(model, optimizer, features, adj, labels, idx_train)\n",
    "        loss_val, acc_val = validate(model, features, adj, labels, idx_val)\n",
    "        if (epoch+1) % 1 == 0: \n",
    "            '''\n",
    "            print('Epoch:{:04d}'.format(epoch+1),\n",
    "                'train',\n",
    "                'loss:{:.3f}'.format(loss_tra),\n",
    "                'acc:{:.2f}'.format(acc_tra*100),\n",
    "                '| val',\n",
    "                'loss:{:.3f}'.format(loss_val),\n",
    "                'acc:{:.2f}'.format(acc_val*100))\n",
    "            '''\n",
    "        if loss_val < best:\n",
    "            best = loss_val\n",
    "            best_epoch = epoch\n",
    "            acc = acc_val\n",
    "            #torch.save(model.state_dict(), checkpt_file)\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "\n",
    "        if bad_counter == patience:\n",
    "            break\n",
    "    \n",
    "    #print(\"Train cost: {:.4f}s\".format(time.time() - t_total))\n",
    "    #print('Load {}th epoch'.format(best_epoch))\n",
    "    #print(\"Val\", \"acc.:{:.1f}\".format(acc*100))\n",
    "\n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'weight_decay': weight_decay,\n",
    "        'dropout': dropout,\n",
    "        'nhid_list': nhid_list,\n",
    "        'accuracy': acc\n",
    "    })\n",
    "\n",
    "# Sort results by accuracy in descending order\n",
    "results = sorted(results, key=lambda x: x['accuracy'], reverse=True)\n",
    "\n",
    "print(\"Hyperparameters and accuracies sorted by accuracy:\")\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee6485-6a08-4187-aa06-c698c959e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [16, 32], 'accuracy': 0.778}\n",
    "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [128, 256, 128], 'accuracy': 0.786}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c6a75c7-f2cd-4fc6-8874-0b5b0c373450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping already run combination: (0.01, 0.0005, 0.5, ())\n",
      "Epoch:0001 train loss:1.966 acc:15.71 | val loss:1.910 acc:21.80\n",
      "Epoch:0002 train loss:1.932 acc:17.86 | val loss:1.904 acc:21.80\n",
      "Epoch:0003 train loss:1.899 acc:22.86 | val loss:1.899 acc:22.60\n",
      "Epoch:0004 train loss:1.866 acc:25.71 | val loss:1.894 acc:23.40\n",
      "Epoch:0005 train loss:1.833 acc:30.71 | val loss:1.889 acc:23.20\n",
      "Epoch:0006 train loss:1.801 acc:37.14 | val loss:1.883 acc:24.60\n",
      "Epoch:0007 train loss:1.770 acc:45.71 | val loss:1.878 acc:26.60\n",
      "Epoch:0008 train loss:1.739 acc:54.29 | val loss:1.872 acc:27.80\n",
      "Epoch:0009 train loss:1.708 acc:62.14 | val loss:1.867 acc:29.40\n",
      "Epoch:0010 train loss:1.678 acc:69.29 | val loss:1.861 acc:31.20\n",
      "Epoch:0011 train loss:1.648 acc:74.29 | val loss:1.856 acc:32.60\n",
      "Epoch:0012 train loss:1.619 acc:80.71 | val loss:1.850 acc:34.40\n",
      "Epoch:0013 train loss:1.591 acc:86.43 | val loss:1.845 acc:35.40\n",
      "Epoch:0014 train loss:1.563 acc:88.57 | val loss:1.840 acc:37.80\n",
      "Epoch:0015 train loss:1.536 acc:92.14 | val loss:1.835 acc:40.20\n",
      "Epoch:0016 train loss:1.509 acc:95.71 | val loss:1.831 acc:42.20\n",
      "Epoch:0017 train loss:1.483 acc:97.14 | val loss:1.826 acc:44.00\n",
      "Epoch:0018 train loss:1.457 acc:97.86 | val loss:1.821 acc:46.20\n",
      "Epoch:0019 train loss:1.432 acc:97.86 | val loss:1.817 acc:46.80\n",
      "Epoch:0020 train loss:1.408 acc:99.29 | val loss:1.812 acc:48.00\n",
      "Epoch:0021 train loss:1.384 acc:99.29 | val loss:1.808 acc:49.40\n",
      "Epoch:0022 train loss:1.361 acc:99.29 | val loss:1.803 acc:50.80\n",
      "Epoch:0023 train loss:1.339 acc:99.29 | val loss:1.799 acc:51.40\n",
      "Epoch:0024 train loss:1.317 acc:100.00 | val loss:1.795 acc:54.40\n",
      "Epoch:0025 train loss:1.296 acc:100.00 | val loss:1.790 acc:55.60\n",
      "Epoch:0026 train loss:1.275 acc:100.00 | val loss:1.786 acc:55.80\n",
      "Epoch:0027 train loss:1.255 acc:100.00 | val loss:1.781 acc:57.60\n",
      "Epoch:0028 train loss:1.235 acc:100.00 | val loss:1.777 acc:59.40\n",
      "Epoch:0029 train loss:1.216 acc:100.00 | val loss:1.773 acc:60.60\n",
      "Epoch:0030 train loss:1.198 acc:100.00 | val loss:1.768 acc:60.60\n",
      "Epoch:0031 train loss:1.180 acc:100.00 | val loss:1.764 acc:61.40\n",
      "Epoch:0032 train loss:1.163 acc:100.00 | val loss:1.759 acc:62.60\n",
      "Epoch:0033 train loss:1.146 acc:100.00 | val loss:1.755 acc:63.40\n",
      "Epoch:0034 train loss:1.130 acc:100.00 | val loss:1.751 acc:65.00\n",
      "Epoch:0035 train loss:1.114 acc:100.00 | val loss:1.747 acc:65.40\n",
      "Epoch:0036 train loss:1.099 acc:100.00 | val loss:1.743 acc:65.60\n",
      "Epoch:0037 train loss:1.084 acc:100.00 | val loss:1.739 acc:66.00\n",
      "Epoch:0038 train loss:1.070 acc:100.00 | val loss:1.735 acc:66.00\n",
      "Epoch:0039 train loss:1.056 acc:100.00 | val loss:1.731 acc:66.20\n",
      "Epoch:0040 train loss:1.042 acc:100.00 | val loss:1.728 acc:66.20\n",
      "Epoch:0041 train loss:1.030 acc:100.00 | val loss:1.724 acc:67.60\n",
      "Epoch:0042 train loss:1.017 acc:100.00 | val loss:1.721 acc:68.20\n",
      "Epoch:0043 train loss:1.005 acc:100.00 | val loss:1.717 acc:68.60\n",
      "Epoch:0044 train loss:0.993 acc:100.00 | val loss:1.714 acc:68.80\n",
      "Epoch:0045 train loss:0.982 acc:100.00 | val loss:1.711 acc:68.80\n",
      "Epoch:0046 train loss:0.971 acc:100.00 | val loss:1.708 acc:69.60\n",
      "Epoch:0047 train loss:0.960 acc:100.00 | val loss:1.705 acc:70.00\n",
      "Epoch:0048 train loss:0.950 acc:100.00 | val loss:1.702 acc:70.20\n",
      "Epoch:0049 train loss:0.940 acc:100.00 | val loss:1.699 acc:70.40\n",
      "Epoch:0050 train loss:0.931 acc:100.00 | val loss:1.696 acc:70.60\n",
      "Epoch:0051 train loss:0.921 acc:100.00 | val loss:1.693 acc:70.20\n",
      "Epoch:0052 train loss:0.912 acc:100.00 | val loss:1.691 acc:70.60\n",
      "Epoch:0053 train loss:0.904 acc:100.00 | val loss:1.688 acc:70.40\n",
      "Epoch:0054 train loss:0.895 acc:100.00 | val loss:1.686 acc:70.40\n",
      "Epoch:0055 train loss:0.887 acc:100.00 | val loss:1.683 acc:70.20\n",
      "Epoch:0056 train loss:0.879 acc:100.00 | val loss:1.681 acc:70.20\n",
      "Epoch:0057 train loss:0.872 acc:100.00 | val loss:1.679 acc:70.20\n",
      "Epoch:0058 train loss:0.864 acc:100.00 | val loss:1.676 acc:70.60\n",
      "Epoch:0059 train loss:0.857 acc:100.00 | val loss:1.674 acc:70.60\n",
      "Epoch:0060 train loss:0.850 acc:100.00 | val loss:1.672 acc:70.60\n",
      "Epoch:0061 train loss:0.843 acc:100.00 | val loss:1.670 acc:70.60\n",
      "Epoch:0062 train loss:0.837 acc:100.00 | val loss:1.668 acc:70.60\n",
      "Epoch:0063 train loss:0.830 acc:100.00 | val loss:1.666 acc:71.00\n",
      "Epoch:0064 train loss:0.824 acc:100.00 | val loss:1.664 acc:71.00\n",
      "Epoch:0065 train loss:0.818 acc:100.00 | val loss:1.662 acc:71.20\n",
      "Epoch:0066 train loss:0.812 acc:100.00 | val loss:1.661 acc:71.40\n",
      "Epoch:0067 train loss:0.806 acc:100.00 | val loss:1.659 acc:71.40\n",
      "Epoch:0068 train loss:0.801 acc:100.00 | val loss:1.657 acc:71.40\n",
      "Epoch:0069 train loss:0.796 acc:100.00 | val loss:1.655 acc:71.20\n",
      "Epoch:0070 train loss:0.790 acc:100.00 | val loss:1.654 acc:71.00\n",
      "Epoch:0071 train loss:0.785 acc:100.00 | val loss:1.652 acc:71.00\n",
      "Epoch:0072 train loss:0.780 acc:100.00 | val loss:1.651 acc:71.00\n",
      "Epoch:0073 train loss:0.775 acc:100.00 | val loss:1.649 acc:71.00\n",
      "Epoch:0074 train loss:0.771 acc:100.00 | val loss:1.647 acc:71.20\n",
      "Epoch:0075 train loss:0.766 acc:100.00 | val loss:1.646 acc:71.20\n",
      "Epoch:0076 train loss:0.762 acc:100.00 | val loss:1.645 acc:71.20\n",
      "Epoch:0077 train loss:0.757 acc:100.00 | val loss:1.643 acc:71.40\n",
      "Epoch:0078 train loss:0.753 acc:100.00 | val loss:1.642 acc:71.40\n",
      "Epoch:0079 train loss:0.749 acc:100.00 | val loss:1.640 acc:71.40\n",
      "Epoch:0080 train loss:0.745 acc:100.00 | val loss:1.639 acc:71.40\n",
      "Epoch:0081 train loss:0.741 acc:100.00 | val loss:1.638 acc:71.40\n",
      "Epoch:0082 train loss:0.737 acc:100.00 | val loss:1.636 acc:71.60\n",
      "Epoch:0083 train loss:0.733 acc:100.00 | val loss:1.635 acc:71.60\n",
      "Epoch:0084 train loss:0.729 acc:100.00 | val loss:1.634 acc:71.80\n",
      "Epoch:0085 train loss:0.726 acc:100.00 | val loss:1.633 acc:72.00\n",
      "Epoch:0086 train loss:0.722 acc:100.00 | val loss:1.632 acc:72.00\n",
      "Epoch:0087 train loss:0.719 acc:100.00 | val loss:1.630 acc:72.00\n",
      "Epoch:0088 train loss:0.715 acc:100.00 | val loss:1.629 acc:72.20\n",
      "Epoch:0089 train loss:0.712 acc:100.00 | val loss:1.628 acc:72.20\n",
      "Epoch:0090 train loss:0.709 acc:100.00 | val loss:1.627 acc:72.20\n",
      "Epoch:0091 train loss:0.706 acc:100.00 | val loss:1.626 acc:72.00\n",
      "Epoch:0092 train loss:0.702 acc:100.00 | val loss:1.625 acc:72.20\n",
      "Epoch:0093 train loss:0.699 acc:100.00 | val loss:1.624 acc:72.20\n",
      "Epoch:0094 train loss:0.696 acc:100.00 | val loss:1.623 acc:72.40\n",
      "Epoch:0095 train loss:0.693 acc:100.00 | val loss:1.622 acc:72.40\n",
      "Epoch:0096 train loss:0.691 acc:100.00 | val loss:1.621 acc:72.60\n",
      "Epoch:0097 train loss:0.688 acc:100.00 | val loss:1.620 acc:72.60\n",
      "Epoch:0098 train loss:0.685 acc:100.00 | val loss:1.619 acc:72.60\n",
      "Epoch:0099 train loss:0.682 acc:100.00 | val loss:1.618 acc:72.60\n",
      "Epoch:0100 train loss:0.679 acc:100.00 | val loss:1.617 acc:72.60\n",
      "Train cost: 2.6735s\n",
      "Load 99th epoch\n",
      "Val acc.:72.6\n",
      "Hyperparameters and accuracies sorted by accuracy:\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [], 'accuracy': 0.726}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [], 'accuracy': 0.72}\n"
     ]
    }
   ],
   "source": [
    "class my_GraphConvolution(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, in_features, out_features,nfeat, bias=True):\n",
    "        super(my_GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features+n+nfeat, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj,x):\n",
    "        \n",
    "        \n",
    "        #support = torch.cat((input-torch.mm(adj.to_dense(), input), adj.to_dense()),1)\n",
    "        \n",
    "        support = torch.cat((torch.mm(adj.to_dense(), input),adj.to_dense(),x),1)\n",
    "        output = torch.mm(support, self.weight)\n",
    "\n",
    "        #output= torch.mm(torch.mm(adj.to_dense(), input) + input, self.weight)\n",
    "\n",
    "       \n",
    "        \n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class my_GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid_list, nclass, dropout):\n",
    "        super(my_GCN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        if nhid_list:\n",
    "            # Input layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nhid_list[0],nfeat))\n",
    "            \n",
    "            # Hidden layers\n",
    "            for i in range(1, len(nhid_list)):\n",
    "                self.layers.append(my_GraphConvolution(nhid_list[i-1], nhid_list[i],nfeat))\n",
    "            \n",
    "            # Output layer\n",
    "            self.layers.append(my_GraphConvolution(nhid_list[-1], nclass,nfeat))\n",
    "        else:\n",
    "            # Single output layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nclass,nfeat))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        h = x.detach().requires_grad_()\n",
    "        \n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            h = F.relu(layer(h, adj,x))\n",
    "            h = F.dropout(h, self.dropout, training=self.training)\n",
    "        \n",
    "        h = self.layers[-1](h, adj,x)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)\n",
    "\n",
    "def train(model, optimizer, features, adj, labels, idx_train):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train].to(device))\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train].to(device))\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    return loss_train.item(), acc_train.item()\n",
    "\n",
    "def validate(model, features, adj, labels, idx_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features, adj)\n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val].to(device))\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val].to(device))\n",
    "        return loss_val.item(), acc_val.item()\n",
    "\n",
    "def test(model, features, adj, labels, idx_test, checkpt_file):\n",
    "    model.load_state_dict(torch.load(checkpt_file))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features, adj)\n",
    "        loss_test = F.nll_loss(output[idx_test], labels[idx_test].to(device))\n",
    "        acc_test = accuracy(output[idx_test], labels[idx_test].to(device))\n",
    "        return loss_test.item(), acc_test.item()\n",
    "        \n",
    "# Hyperparameters\n",
    "nfeat = features.shape[1]\n",
    "nclass = num_classes\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hyperparams = {\n",
    "    'lr': [0.01],\n",
    "    'weight_decay': [5e-4],\n",
    "    'dropout': [0.5,0.3],\n",
    "    'nhid_list': [[]],\n",
    "}\n",
    "\n",
    "def load_previous_results(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return []\n",
    "\n",
    "def save_results(file_path, results):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "# File paths for saving results and combinations\n",
    "results_file_path = 'results.pkl'\n",
    "combinations_file_path = 'combinations.pkl'\n",
    "\n",
    "# To store results\n",
    "results = []\n",
    "\n",
    "# To store already run combinations\n",
    "run_combinations = set()\n",
    "\n",
    "# Load previously run combinations if available\n",
    "# For simplicity, assuming results is a list of dicts loaded from a file\n",
    "results = load_previous_results(results_file_path)\n",
    "for result in results:\n",
    "     run_combinations.add((result['lr'], result['weight_decay'], result['dropout'], tuple(result['nhid_list'])))\n",
    "\n",
    "for lr, weight_decay, dropout, nhid_list in itertools.product(\n",
    "    hyperparams['lr'], hyperparams['weight_decay'], hyperparams['dropout'], hyperparams['nhid_list']\n",
    "):\n",
    "    combination = (lr, weight_decay, dropout, tuple(nhid_list))\n",
    "    \n",
    "    if combination in run_combinations:\n",
    "        print(f\"Skipping already run combination: {combination}\")\n",
    "        continue\n",
    "    \n",
    "    run_combinations.add(combination)\n",
    "    \n",
    "    model = my_GCN(nfeat, nhid_list, nclass, dropout)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    epochs = 100\n",
    "    patience = 10\n",
    "    best = 999999999\n",
    "    best_epoch = 0\n",
    "    acc = 0\n",
    "    t_total = time.time()\n",
    "    bad_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss_tra, acc_tra = train(model, optimizer, features, adj, labels, idx_train)\n",
    "        loss_val, acc_val = validate(model, features, adj, labels, idx_val)\n",
    "        if (epoch+1) % 1 == 0: \n",
    "            \n",
    "            print('Epoch:{:04d}'.format(epoch+1),\n",
    "                'train',\n",
    "                'loss:{:.3f}'.format(loss_tra),\n",
    "                'acc:{:.2f}'.format(acc_tra*100),\n",
    "                '| val',\n",
    "                'loss:{:.3f}'.format(loss_val),\n",
    "                'acc:{:.2f}'.format(acc_val*100))\n",
    "            \n",
    "        if loss_val < best:\n",
    "            best = loss_val\n",
    "            best_epoch = epoch\n",
    "            acc = acc_val\n",
    "            #torch.save(model.state_dict(), checkpt_file)\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "\n",
    "        if bad_counter == patience:\n",
    "            break\n",
    "    \n",
    "    print(\"Train cost: {:.4f}s\".format(time.time() - t_total))\n",
    "    print('Load {}th epoch'.format(best_epoch))\n",
    "    print(\"Val\", \"acc.:{:.1f}\".format(acc*100))\n",
    "\n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'weight_decay': weight_decay,\n",
    "        'dropout': dropout,\n",
    "        'nhid_list': nhid_list,\n",
    "        'accuracy': acc\n",
    "    })\n",
    "\n",
    "# Sort results by accuracy in descending order\n",
    "results = sorted(results, key=lambda x: x['accuracy'], reverse=True)\n",
    "\n",
    "print(\"Hyperparameters and accuracies sorted by accuracy:\")\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "# Save results and run_combinations for future use\n",
    "save_results(results_file_path, results)\n",
    "save_results(combinations_file_path, list(run_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e4f28a6-47bb-4aae-a7b3-c7b941a071e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1557039151.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(support.shape)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(adj.to_dense().shape)\n",
    "        print(support.shape)\n",
    "        print(output.shape)\n",
    "\n",
    "\n",
    "torch.Size([2708, 2708]) adj\n",
    "torch.Size([2708, 16])  support= input x weight\n",
    "torch.Size([2708, 2724])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
