{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62169ce6-d3ff-486e-8af5-4fc9a5c00eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from model import *\n",
    "from process import *\n",
    "import pickle\n",
    "import itertools\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ddbba5-f6a5-4367-837b-bcdea631a5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 2708])\n",
      "torch.Size([2708, 1433])\n",
      "torch.Size([2708])\n",
      "torch.Size([140])\n",
      "torch.Size([500])\n",
      "torch.Size([1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayday\\Desktop\\Master_Thesis\\experiment\\utils.py:87: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n"
     ]
    }
   ],
   "source": [
    "data='cora'\n",
    "adj, features, labels,idx_train,idx_val,idx_test = load_citation(data)\n",
    "#splitstr = 'splits/'+data+'_split_0.6_0.2_'+str(0)+'.npz'\n",
    "#adj, features, labels, idx_train, idx_val, idx_test, num_features, num_labels = full_load_data(data,splitstr)\n",
    "print(adj.shape)\n",
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "print(idx_train.shape)\n",
    "print(idx_val.shape)\n",
    "print(idx_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fff0d170-b89a-4eb7-a2a1-68559e842d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "diagonal = np.diag(adj.to_dense().cpu())\n",
    "print(np.any(diagonal != 0 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb6ef84e-d5ff-41a6-925b-1e339edf8b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25      , 0.25      , 0.16666667, ..., 0.5       , 0.2       ,\n",
       "       0.2       ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f516b9ec-672b-4a67-99b1-bbf1323a7f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudaid = \"cuda\"\n",
    "device = torch.device(cudaid)\n",
    "features = features.to(device)\n",
    "adj = adj.to(device).coalesce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42522538-1af7-4041-894c-3100f095ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=features.shape[0]\n",
    "feature_size=features.shape[1]\n",
    "num_classes = len(torch.unique(labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e25c24-4159-4e80-92d0-987042226f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee82cd-1ba7-4682-8c7f-2a8ff358a71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddf9e80-fea2-443e-93b7-ded05b3e327d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf9578-d717-4873-a300-cfd4d01f74b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6477916-3c0a-42a5-925c-7ca5bd3c39e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m [\u001b[43mAH\u001b[49m,A,X]\n\u001b[0;32m      2\u001b[0m results\n\u001b[0;32m      3\u001b[0m combinations\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AH' is not defined"
     ]
    }
   ],
   "source": [
    "[AH,A,X]\n",
    "results\n",
    "combinations\n",
    "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.4, 'nhid_list': [128, 512, 128], 'accuracy': 0.792}\n",
    "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [16, 32], 'accuracy': 0.778}\n",
    "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [128, 256, 128], 'accuracy': 0.786}\n",
    "\n",
    "'''\n",
    "class my_GraphConvolution(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, in_features, out_features,nfeat, bias=True):\n",
    "        super(my_GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features+n+nfeat, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj,x):\n",
    "        \n",
    "        \n",
    "        #support = torch.cat((input-torch.mm(adj.to_dense(), input), adj.to_dense()),1)\n",
    "        \n",
    "        support = torch.cat((torch.mm(adj.to_dense(), input),adj.to_dense(),x),1)\n",
    "        output = torch.mm(support, self.weight)\n",
    "\n",
    "        #output= torch.mm(torch.mm(adj.to_dense(), input) + input, self.weight)\n",
    "\n",
    "       \n",
    "        \n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class my_GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid_list, nclass, dropout):\n",
    "        super(my_GCN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        if nhid_list:\n",
    "            # Input layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nhid_list[0],nfeat))\n",
    "            \n",
    "            # Hidden layers\n",
    "            for i in range(1, len(nhid_list)):\n",
    "                self.layers.append(my_GraphConvolution(nhid_list[i-1], nhid_list[i],nfeat))\n",
    "            \n",
    "            # Output layer\n",
    "            self.layers.append(my_GraphConvolution(nhid_list[-1], nclass,nfeat))\n",
    "        else:\n",
    "            # Single output layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nclass,nfeat))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        h = x.detach().requires_grad_()\n",
    "        \n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            h = F.relu(layer(h, adj,x))\n",
    "            h = F.dropout(h, self.dropout, training=self.training)\n",
    "        \n",
    "        h = self.layers[-1](h, adj,x)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee6485-6a08-4187-aa06-c698c959e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[H-AH, A, X]\n",
    "{'lr': 0.05, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [], 'accuracy': 0.64}\n",
    "results_1\n",
    "intersting all the good acc without hidden layer\n",
    "'''\n",
    "class my_GraphConvolution(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, in_features, out_features,nfeat, bias=True):\n",
    "        super(my_GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features+n+nfeat, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj,x):\n",
    "        \n",
    "        \n",
    "        #support = torch.cat((input-torch.mm(adj.to_dense(), input), adj.to_dense()),1)\n",
    "        \n",
    "        #support = torch.cat((torch.mm(adj.to_dense(), input),adj.to_dense(),x),1)\n",
    "        #output = torch.mm(support, self.weight)\n",
    "\n",
    "        #output= torch.mm(torch.mm(adj.to_dense(), input) + input, self.weight)\n",
    "\n",
    "        support = torch.cat((input - torch.mm(adj.to_dense(), input),adj.to_dense(),x),1)\n",
    "        output = torch.mm(support, self.weight)\n",
    "\n",
    "       \n",
    "        \n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class my_GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid_list, nclass, dropout):\n",
    "        super(my_GCN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        if nhid_list:\n",
    "            # Input layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nhid_list[0],nfeat))\n",
    "            \n",
    "            # Hidden layers\n",
    "            for i in range(1, len(nhid_list)):\n",
    "                self.layers.append(my_GraphConvolution(nhid_list[i-1], nhid_list[i],nfeat))\n",
    "            \n",
    "            # Output layer\n",
    "            self.layers.append(my_GraphConvolution(nhid_list[-1], nclass,nfeat))\n",
    "        else:\n",
    "            # Single output layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nclass,nfeat))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        h = x.detach().requires_grad_()\n",
    "        \n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            h = F.relu(layer(h, adj,x))\n",
    "            h = F.dropout(h, self.dropout, training=self.training)\n",
    "        \n",
    "        h = self.layers[-1](h, adj,x)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a677f569-494a-44a2-9504-74dd7b86ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AH+H\n",
    "skip connection\n",
    "results_2\n",
    "{'lr': 0.001, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [512, 1024, 512], 'accuracy': 0.8200000000000001}\n",
    "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [256, 512], 'accuracy': 0.81}\n",
    "'''\n",
    "class my_GraphConvolution(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, in_features, out_features,nfeat, bias=True):\n",
    "        super(my_GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj,x):\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        support = torch.mm(adj.to_dense(), input) + input\n",
    "        output = torch.mm(support, self.weight)\n",
    "\n",
    "\n",
    "       \n",
    "        \n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class my_GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid_list, nclass, dropout):\n",
    "        super(my_GCN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        if nhid_list:\n",
    "            # Input layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nhid_list[0],nfeat))\n",
    "            \n",
    "            # Hidden layers\n",
    "            for i in range(1, len(nhid_list)):\n",
    "                self.layers.append(my_GraphConvolution(nhid_list[i-1], nhid_list[i],nfeat))\n",
    "            \n",
    "            # Output layer\n",
    "            self.layers.append(my_GraphConvolution(nhid_list[-1], nclass,nfeat))\n",
    "        else:\n",
    "            # Single output layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nclass,nfeat))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        h = x.detach().requires_grad_()\n",
    "        \n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            h = F.relu(layer(h, adj,x))\n",
    "            h = F.dropout(h, self.dropout, training=self.training)\n",
    "        \n",
    "        h = self.layers[-1](h, adj,x)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17ca7a-f77a-4d85-89d2-2eb32b47a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "[X,AH+H]\n",
    "{'lr': 0.001, 'weight_decay': 0.001, 'dropout': 0.4, 'nhid_list': [128, 512, 128], 'accuracy': 0.812}\n",
    "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [64, 128, 64], 'accuracy': 0.81}\n",
    "results3\n",
    "'''\n",
    "class my_GraphConvolution(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, in_features, out_features,nfeat, bias=True):\n",
    "        super(my_GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(nfeat+in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj,x):\n",
    "        \n",
    "        \n",
    "        #support = torch.cat((input-torch.mm(adj.to_dense(), input), adj.to_dense()),1)\n",
    "        \n",
    "        support = torch.cat((x,torch.mm(adj.to_dense(), input) + input),1)\n",
    "        output = torch.mm(support, self.weight)\n",
    "\n",
    "        #output= torch.mm(torch.mm(adj.to_dense(), input) + input, self.weight)\n",
    "\n",
    "       \n",
    "        \n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class my_GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid_list, nclass, dropout):\n",
    "        super(my_GCN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        if nhid_list:\n",
    "            # Input layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nhid_list[0],nfeat))\n",
    "            \n",
    "            # Hidden layers\n",
    "            for i in range(1, len(nhid_list)):\n",
    "                self.layers.append(my_GraphConvolution(nhid_list[i-1], nhid_list[i],nfeat))\n",
    "            \n",
    "            # Output layer\n",
    "            self.layers.append(my_GraphConvolution(nhid_list[-1], nclass,nfeat))\n",
    "        else:\n",
    "            # Single output layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nclass,nfeat))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        h = x.detach().requires_grad_()\n",
    "        \n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            h = F.relu(layer(h, adj,x))\n",
    "            h = F.dropout(h, self.dropout, training=self.training)\n",
    "        \n",
    "        h = self.layers[-1](h, adj,x)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16ab8d3f-5aaa-48b6-9a9d-3bc67037110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[AH,X]\n",
    "\n",
    "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [512, 1024, 512], 'accuracy': 0.8180000000000001}\n",
    "class my_GraphConvolution(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, in_features, out_features,nfeat, bias=True):\n",
    "        super(my_GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(nfeat+in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj,x):\n",
    "        \n",
    "        \n",
    "        #support = torch.cat((input-torch.mm(adj.to_dense(), input), adj.to_dense()),1)\n",
    "        \n",
    "        support = torch.cat((torch.mm(adj.to_dense(), input),x),1)\n",
    "        output = torch.mm(support, self.weight)\n",
    "\n",
    "        #output= torch.mm(torch.mm(adj.to_dense(), input) + input, self.weight)\n",
    "\n",
    "       \n",
    "        \n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class my_GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid_list, nclass, dropout):\n",
    "        super(my_GCN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        if nhid_list:\n",
    "            # Input layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nhid_list[0],nfeat))\n",
    "            \n",
    "            # Hidden layers\n",
    "            for i in range(1, len(nhid_list)):\n",
    "                self.layers.append(my_GraphConvolution(nhid_list[i-1], nhid_list[i],nfeat))\n",
    "            \n",
    "            # Output layer\n",
    "            self.layers.append(my_GraphConvolution(nhid_list[-1], nclass,nfeat))\n",
    "        else:\n",
    "            # Single output layer\n",
    "            self.layers.append(my_GraphConvolution(int(round(nfeat)), nclass,nfeat))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        h = x.detach().requires_grad_()\n",
    "        \n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            h = F.relu(layer(h, adj,x))\n",
    "            h = F.dropout(h, self.dropout, training=self.training)\n",
    "        \n",
    "        h = self.layers[-1](h, adj,x)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c6a75c7-f2cd-4fc6-8874-0b5b0c373450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters and accuracies sorted by accuracy:\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [512, 1024, 512], 'accuracy': 0.8180000000000001}\n",
      "{'lr': 0.05, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [256, 512], 'accuracy': 0.8140000000000001}\n",
      "{'lr': 0.05, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [128, 256, 128], 'accuracy': 0.812}\n",
      "{'lr': 0.05, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [256, 512], 'accuracy': 0.808}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [128, 256, 128], 'accuracy': 0.808}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [256, 512], 'accuracy': 0.806}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [256, 512], 'accuracy': 0.806}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [256, 512], 'accuracy': 0.804}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [512, 1024, 512], 'accuracy': 0.804}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [256, 512], 'accuracy': 0.804}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [64, 128, 64], 'accuracy': 0.804}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [256, 512], 'accuracy': 0.802}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [256, 512], 'accuracy': 0.802}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [512, 1024, 512], 'accuracy': 0.8}\n",
      "{'lr': 0.05, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [64, 128, 64], 'accuracy': 0.798}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [64, 128, 64], 'accuracy': 0.798}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [512, 1024, 512], 'accuracy': 0.798}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [512, 1024, 512], 'accuracy': 0.796}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [128, 256, 128], 'accuracy': 0.796}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [512, 1024, 512], 'accuracy': 0.796}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [64, 128, 64], 'accuracy': 0.792}\n",
      "{'lr': 0.05, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [128, 256, 128], 'accuracy': 0.79}\n",
      "{'lr': 0.05, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [512, 1024, 512], 'accuracy': 0.79}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [512, 1024, 512], 'accuracy': 0.79}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [64, 128, 64], 'accuracy': 0.79}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [16, 32, 16], 'accuracy': 0.788}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [64, 128, 64], 'accuracy': 0.788}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [512, 1024, 512], 'accuracy': 0.788}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [256, 512], 'accuracy': 0.786}\n",
      "{'lr': 0.05, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [64, 128, 64], 'accuracy': 0.784}\n",
      "{'lr': 0.05, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [64, 128, 64], 'accuracy': 0.784}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [256, 512], 'accuracy': 0.784}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [128, 256, 128], 'accuracy': 0.784}\n",
      "{'lr': 0.05, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [512, 1024, 512], 'accuracy': 0.782}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [16, 32, 16], 'accuracy': 0.782}\n",
      "{'lr': 0.05, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [256, 512], 'accuracy': 0.78}\n",
      "{'lr': 0.05, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [512, 1024, 512], 'accuracy': 0.78}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [128, 256, 128], 'accuracy': 0.776}\n",
      "{'lr': 0.05, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [128, 256, 128], 'accuracy': 0.772}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [128, 256, 128], 'accuracy': 0.772}\n",
      "{'lr': 0.05, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [64, 128, 64], 'accuracy': 0.77}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [128, 256, 128], 'accuracy': 0.77}\n",
      "{'lr': 0.05, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [512, 1024, 512], 'accuracy': 0.766}\n",
      "{'lr': 0.05, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [256, 512], 'accuracy': 0.766}\n",
      "{'lr': 0.05, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [16, 32, 16], 'accuracy': 0.764}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [128, 256, 128], 'accuracy': 0.762}\n",
      "{'lr': 0.05, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [16, 32, 16], 'accuracy': 0.76}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [64, 128, 64], 'accuracy': 0.756}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [16, 32, 16], 'accuracy': 0.754}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [128, 256, 128], 'accuracy': 0.752}\n",
      "{'lr': 0.05, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [128, 256, 128], 'accuracy': 0.75}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [64, 128, 64], 'accuracy': 0.7020000000000001}\n",
      "{'lr': 0.05, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [16, 32, 16], 'accuracy': 0.6980000000000001}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [64, 128, 64], 'accuracy': 0.6960000000000001}\n",
      "{'lr': 0.05, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [], 'accuracy': 0.686}\n",
      "{'lr': 0.05, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [16, 32, 16], 'accuracy': 0.686}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [], 'accuracy': 0.686}\n",
      "{'lr': 0.05, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [], 'accuracy': 0.684}\n",
      "{'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [], 'accuracy': 0.684}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.3, 'nhid_list': [], 'accuracy': 0.682}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [], 'accuracy': 0.662}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [], 'accuracy': 0.646}\n",
      "{'lr': 0.05, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [], 'accuracy': 0.636}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [], 'accuracy': 0.618}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [], 'accuracy': 0.614}\n",
      "{'lr': 0.05, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [], 'accuracy': 0.5760000000000001}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [16, 32, 16], 'accuracy': 0.554}\n",
      "{'lr': 0.01, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [16, 32, 16], 'accuracy': 0.5}\n",
      "{'lr': 0.005, 'weight_decay': 0.0005, 'dropout': 0.5, 'nhid_list': [16, 32, 16], 'accuracy': 0.256}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.3, 'nhid_list': [16, 32, 16], 'accuracy': 0.224}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [16, 32, 16], 'accuracy': 0.182}\n",
      "{'lr': 0.005, 'weight_decay': 0.005, 'dropout': 0.5, 'nhid_list': [], 'accuracy': 0.132}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train(model, optimizer, features, adj, labels, idx_train):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train].to(device))\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train].to(device))\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    return loss_train.item(), acc_train.item()\n",
    "\n",
    "def validate(model, features, adj, labels, idx_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features, adj)\n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val].to(device))\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val].to(device))\n",
    "        return loss_val.item(), acc_val.item()\n",
    "\n",
    "def test(model, features, adj, labels, idx_test, checkpt_file):\n",
    "    model.load_state_dict(torch.load(checkpt_file))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features, adj)\n",
    "        loss_test = F.nll_loss(output[idx_test], labels[idx_test].to(device))\n",
    "        acc_test = accuracy(output[idx_test], labels[idx_test].to(device))\n",
    "        return loss_test.item(), acc_test.item()\n",
    "        \n",
    "# Hyperparameters\n",
    "nfeat = features.shape[1]\n",
    "nclass = num_classes\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "'''\n",
    "hyperparams = {\n",
    "    'lr': [0.05, 0.01, 0.005, 0.001],\n",
    "    'weight_decay': [5e-4, 5e-3, 1e-4, 1e-3],\n",
    "    'dropout': [0.5, 0.3, 0.4, 0.2],\n",
    "    'nhid_list': [[], [256, 512], [16, 32, 16], [64, 128, 64], [128, 256, 128], [512, 1024, 512], [32, 64, 32], [128, 512, 128]]\n",
    "}\n",
    "'''\n",
    "hyperparams = {\n",
    "    'lr': [0.05, 0.01, 0.005],\n",
    "    'weight_decay': [5e-4, 5e-3],\n",
    "    'dropout': [0.5, 0.3],\n",
    "    'nhid_list': [[], [256, 512], [16, 32, 16], [64, 128, 64], [128, 256, 128], [512, 1024, 512]]\n",
    "}\n",
    "\n",
    "def load_previous_results(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return []\n",
    "\n",
    "def save_results(file_path, results):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "# File paths for saving results and combinations\n",
    "results_file_path = 'results_4.pkl'\n",
    "combinations_file_path = 'combinations_4.pkl'\n",
    "\n",
    "# To store results\n",
    "results = []\n",
    "\n",
    "# To store already run combinations\n",
    "run_combinations = set()\n",
    "\n",
    "# Load previously run combinations if available\n",
    "# For simplicity, assuming results is a list of dicts loaded from a file\n",
    "results = load_previous_results(results_file_path)\n",
    "for result in results:\n",
    "     run_combinations.add((result['lr'], result['weight_decay'], result['dropout'], tuple(result['nhid_list'])))\n",
    "\n",
    "for lr, weight_decay, dropout, nhid_list in itertools.product(\n",
    "    hyperparams['lr'], hyperparams['weight_decay'], hyperparams['dropout'], hyperparams['nhid_list']\n",
    "):\n",
    "    combination = (lr, weight_decay, dropout, tuple(nhid_list))\n",
    "    \n",
    "    if combination in run_combinations:\n",
    "        print(f\"Skipping already run combination: {combination}\")\n",
    "        continue\n",
    "    \n",
    "    run_combinations.add(combination)\n",
    "    \n",
    "    model = my_GCN(nfeat, nhid_list, nclass, dropout)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    epochs = 100\n",
    "    patience = 10\n",
    "    best = 999999999\n",
    "    best_epoch = 0\n",
    "    acc = 0\n",
    "    t_total = time.time()\n",
    "    bad_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss_tra, acc_tra = train(model, optimizer, features, adj, labels, idx_train)\n",
    "        loss_val, acc_val = validate(model, features, adj, labels, idx_val)\n",
    "        if (epoch+1) % 1 == 0: \n",
    "            '''\n",
    "            print('Epoch:{:04d}'.format(epoch+1),\n",
    "                'train',\n",
    "                'loss:{:.3f}'.format(loss_tra),\n",
    "                'acc:{:.2f}'.format(acc_tra*100),\n",
    "                '| val',\n",
    "                'loss:{:.3f}'.format(loss_val),\n",
    "                'acc:{:.2f}'.format(acc_val*100))\n",
    "            '''\n",
    "        if loss_val < best:\n",
    "            best = loss_val\n",
    "            best_epoch = epoch\n",
    "            acc = acc_val\n",
    "            #torch.save(model.state_dict(), checkpt_file)\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "\n",
    "        if bad_counter == patience:\n",
    "            break\n",
    "    \n",
    "    #print(\"Train cost: {:.4f}s\".format(time.time() - t_total))\n",
    "    #print('Load {}th epoch'.format(best_epoch))\n",
    "    #print(\"Val\", \"acc.:{:.1f}\".format(acc*100))\n",
    "\n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'weight_decay': weight_decay,\n",
    "        'dropout': dropout,\n",
    "        'nhid_list': nhid_list,\n",
    "        'accuracy': acc\n",
    "    })\n",
    "\n",
    "# Sort results by accuracy in descending order\n",
    "results = sorted(results, key=lambda x: x['accuracy'], reverse=True)\n",
    "\n",
    "print(\"Hyperparameters and accuracies sorted by accuracy:\")\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "# Save results and run_combinations for future use\n",
    "#save_results(results_file_path, results)\n",
    "#save_results(combinations_file_path, list(run_combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f28a6-47bb-4aae-a7b3-c7b941a071e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
